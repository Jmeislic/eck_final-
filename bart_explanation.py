import pandas as pd
import torch
from datasets import load_dataset, Dataset
from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments
from torch.utils.data import Dataset as TorchDataset
import os
from sklearn.model_selection import train_test_split


#The start of this code was generated by asking google gemmini ""Can you create for me a python program which trains a custom instance of the AI model BART, which creates a sentence describing why an action was moral or immoral from a specified csv. The csv contains 8 collums, "norm" "situation" "intention" "moral_action" "moral_consequence" "immoral_action" "immoral_consequence" and "label". For each row of the dataset it should check if the label is 0 which means unethical and 1 if it is ethical. Then create a sentence with the situation combined with the corresponding action. This should then correlate to the corresponding consequence, being the 2nd part of the training model."

# --- Configuration ---
# You might want to use a smaller BART model for faster training/testing, e.g., 'facebook/bart-base'
MODEL_NAME = 'facebook/bart-large'
CSV_FILE_PATH = f"moral_stories_csv/data_classification_action+context+consequence_lexical_bias_train.csv"
OUTPUT_DIR = './bart_ethics_finetuned'
BATCH_SIZE = 4  # Start small, especially with bart-large
NUM_TRAIN_EPOCHS = 3
MAX_INPUT_LENGTH = 128
MAX_TARGET_LENGTH = 64

# --- Custom Dataset Class ---
class EthicsConsequenceDataset(TorchDataset):
    """
    A custom PyTorch Dataset for loading and processing the ethical data.
    """
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # Create the dictionary for the model
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        # Add the labels (the consequence tokens)
        item['labels'] = torch.tensor(self.labels['input_ids'][idx])
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

# --- Data Preparation Function ---
def prepare_data(csv_path, tokenizer):
    """Loads the CSV, preprocesses the data into input/target pairs, and tokenizes."""
    print(f"Loading data from {csv_path}...")
    try:
        df = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f"Error: The file {csv_path} was not found.")
        return None, None

    # 1. Create the input text (Situation + Action) and target text (Consequence)
    inputs = []
    targets = []
    
    # Iterate through the DataFrame to construct the required input/target strings
    for index, row in df.iterrows():
        situation = row['situation']
        label = row['label']
        
        # Determine the correct action and consequence based on the label (0=Immoral, 1=Moral)
        if label == 1: # Ethical/Moral
            action = row['moral_action']
            consequence = row['moral_consequence']
            
            # The model is asked to describe *why* an action was moral/immoral.
            # We structure the input as: "[SITUATION]. [ACTION]. It was ethical because:"
            input_text = f"{situation}. {action}. It was ethical because:"
            
        elif label == 0: # Unethical/Immoral
            action = row['immoral_action']
            consequence = row['immoral_consequence']
            
            # Structure the input as: "[SITUATION]. [ACTION]. It was unethical because:"
            input_text = f"{situation}. {action}. It was unethical because:"
        else:
            # Skip rows with an unknown label
            continue

        inputs.append(input_text)
        # The target/label is the consequence which explains the justification
        targets.append(consequence) 

    print(f"Total processed samples: {len(inputs)}")

    # 2. Split data into training and validation sets
    train_inputs, val_inputs, train_targets, val_targets = train_test_split(
        inputs, targets, test_size=0.1, random_state=42
    )

    # 3. Tokenize the input and target texts
    print("Tokenizing data...")
    train_input_encodings = tokenizer(
        train_inputs, truncation=True, padding='max_length', max_length=MAX_INPUT_LENGTH
    )
    val_input_encodings = tokenizer(
        val_inputs, truncation=True, padding='max_length', max_length=MAX_INPUT_LENGTH
    )

    train_target_encodings = tokenizer(
        train_targets, truncation=True, padding='max_length', max_length=MAX_TARGET_LENGTH
    )
    val_target_encodings = tokenizer(
        val_targets, truncation=True, padding='max_length', max_length=MAX_TARGET_LENGTH
    )

    # 4. Create PyTorch Datasets
    train_dataset = EthicsConsequenceDataset(train_input_encodings, train_target_encodings)
    val_dataset = EthicsConsequenceDataset(val_input_encodings, val_target_encodings)

    return train_dataset, val_dataset

# --- Main Fine-Tuning Function ---
def fine_tune_bart():
    """Initializes and runs the BART fine-tuning process."""
    print(f"Loading BART tokenizer and model: {MODEL_NAME}...")
    tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)
    model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)

    # Prepare the datasets
    train_dataset, val_dataset = prepare_data(CSV_FILE_PATH, tokenizer)

    if train_dataset is None:
        return

    # 1. Define Training Arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_TRAIN_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=100,
        #To fix this line I asked Gemini Can you help with this error model.safetensors:  15%|                                                                                                                            | 149M/1.02G [00:06<00:20, 43.3MB/s]Traceback (most recent call last):  File "C:\Users\benra\383\eck_final-\bart_explanation.py", line 207, in <module>    fine_tune_bart()    ~~~~~~~~~~~~~~^^  File "C:\Users\benra\383\eck_final-\bart_explanation.py", line 126, in fine_tune_bart    training_args = TrainingArguments(        output_dir=OUTPUT_DIR,    ...<11 lines>...        report_to="none" # Disable logging to external services for simplicity    )TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy
        eval_strategy="epoch", # Evaluate at the end of each epoch
        save_strategy="epoch",       # Save checkpoint at the end of each epoch
        load_best_model_at_end=True, # Load the best model found
        fp16=torch.cuda.is_available(), # Use mixed precision if CUDA is available
        report_to="none" # Disable logging to external services for simplicity
    )

    # 2. Initialize the Hugging Face Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
    )

    # 3. Start Training
    print("\n" + "="*50)
    print("ðŸš€ Starting BART Fine-Tuning...")
    print("="*50)
    trainer.train()

    # 4. Save the Final Model and Tokenizer
    print("\nâœ… Training complete. Saving final model and tokenizer...")
    trainer.save_model(os.path.join(OUTPUT_DIR, "final"))
    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final"))
    print(f"Model saved to: {os.path.join(OUTPUT_DIR, 'final')}")


# --- Inference (Example of how to use the trained model) ---
def generate_justification(text_prompt, model_path=os.path.join(OUTPUT_DIR, 'final')):
    """Loads the trained model and generates a consequence/justification."""
    
   
    try:
        tokenizer = BartTokenizer.from_pretrained(model_path)
        model = BartForConditionalGeneration.from_pretrained(model_path)
        
        # Move model to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)

        inputs = tokenizer(
            text_prompt, 
            return_tensors="pt", 
            max_length=MAX_INPUT_LENGTH, 
            truncation=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        print("\nGenerating justification...")
        # Use beam search for better generation quality
        generated_ids = model.generate(
            **inputs, 
            max_length=MAX_TARGET_LENGTH, 
            num_beams=4, 
            early_stopping=True
        )

        output_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)
        return output_text
        
    except Exception as e:
        print(f"Could not load model for inference or generate text. Error: {e}")
        return None

# --- Execution ---
if __name__ == "__main__":
    # Ensure your 'your_ethical_data.csv' file is in the same directory or specify the full path.
    # To run this successfully, you must have your data file ready.
    # For demonstration, you might want to uncomment the line below to create a dummy CSV:
    # create_dummy_csv(CSV_FILE_PATH) 
    
    # Run the fine-tuning process
    fine_tune_bart()
    
    # --- Example Inference (Requires successful training) ---
    print("\n" + "="*50)
    print("ðŸ”¬ Testing Trained Model")
    print("="*50)

    # Example prompt for a moral action: 
    # "A child found a wallet on the street. The child returned the wallet to the police. It was ethical because:"
    # The model should generate the consequence/justification.
    test_input_moral = (
        "A cashier undercharged a customer by mistake. The customer walked out without saying anything. "
        "It was unethical because:"
    )
    justification = generate_justification(test_input_moral)

    if justification:
        print(f"\nPrompt: {test_input_moral}")
        print(f"Justification: **{justification}**")